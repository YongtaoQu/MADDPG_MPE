{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cd73102-d130-4eec-b884-4ee49035dee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from pettingzoo.mpe import simple_spread_v3,simple_speaker_listener_v4,simple_adversary_v3,simple_world_comm_v3\n",
    "from tqdm import trange\n",
    "import json\n",
    "from agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\n",
    "from agilerl.hpo.mutation import Mutations\n",
    "from agilerl.hpo.tournament import TournamentSelection\n",
    "from agilerl.utils.utils import initialPopulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e64e1934-36d8-40c2-b68c-321023d69ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, env, idx, results):\n",
    "    env.reset()\n",
    "    # Append number of agents and agent IDs to the initial hyperparameter dictionary\n",
    "    n_agents = env.num_agents\n",
    "    agent_ids = env.agents\n",
    "\n",
    "    # Define test loop parameters\n",
    "    episodes = 100  # Number of episodes to test agent on\n",
    "    max_steps = 25  # Max number of steps to take in the environment in each episode\n",
    "\n",
    "    rewards = []  # List to collect total episodic reward\n",
    "    indi_agent_rewards = {\n",
    "        agent_id: [] for agent_id in agent_ids\n",
    "    }  # Dictionary to collect inidivdual agent rewards\n",
    "\n",
    "    # Test loop for inference\n",
    "    for ep in range(episodes):\n",
    "        state, info = env.reset()\n",
    "        agent_reward = {agent_id: 0 for agent_id in agent_ids}\n",
    "        score = 0\n",
    "        for _ in range(max_steps):\n",
    "            agent_mask = info[\"agent_mask\"] if \"agent_mask\" in info.keys() else None\n",
    "            env_defined_actions = (\n",
    "                info[\"env_defined_actions\"]\n",
    "                if \"env_defined_actions\" in info.keys()\n",
    "                else None\n",
    "            )\n",
    "\n",
    "            # Get next action from agent\n",
    "            cont_actions, discrete_action = model.getAction(\n",
    "                state,\n",
    "                epsilon=0,\n",
    "                agent_mask=agent_mask,\n",
    "                env_defined_actions=env_defined_actions,\n",
    "            )\n",
    "            if model.discrete_actions:\n",
    "                action = discrete_action\n",
    "            else:\n",
    "                action = cont_actions\n",
    "\n",
    "            # Take action in environment\n",
    "            state, reward, termination, truncation, info = env.step(action)\n",
    "\n",
    "            # Save agent's reward for this step in this episode\n",
    "            for agent_id, r in reward.items():\n",
    "                agent_reward[agent_id] += r\n",
    "\n",
    "            # Determine total score for the episode and then append to rewards list\n",
    "            score = sum(agent_reward.values())\n",
    "\n",
    "            # Stop episode if any agents have terminated\n",
    "            if any(truncation.values()) or any(termination.values()):\n",
    "                break\n",
    "\n",
    "        rewards.append(score)\n",
    "\n",
    "        # Record agent specific episodic reward\n",
    "        for agent_id in agent_ids:\n",
    "            indi_agent_rewards[agent_id].append(agent_reward[agent_id])\n",
    "    env.close()\n",
    "    \n",
    "    output_dict = {\n",
    "        \"EpisodeIndex\": idx,\n",
    "        \"EpisodeReward\": rewards,\n",
    "        \"AgentReward\": indi_agent_rewards,\n",
    "    }\n",
    "    results.append(output_dict)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2881d656-3db1-48f2-9916-26b19ff3374b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11050). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== AgileRL Online Multi-Agent Demo =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 19/1000 [02:02<58:56,  3.60s/it]  /root/miniconda3/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/root/miniconda3/lib/python3.8/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 12%|█▏        | 120/1000 [13:20<1:12:58,  4.98s/it]"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"===== AgileRL Online Multi-Agent Demo =====\")\n",
    "\n",
    "    # Define the network configuration\n",
    "    NET_CONFIG = {\n",
    "        \"arch\": \"mlp\",  # Network architecture\n",
    "        \"h_size\": [64, 64],  # Actor hidden size\n",
    "    }\n",
    "\n",
    "    # Define the initial hyperparameters\n",
    "    INIT_HP = {\n",
    "        \"POPULATION_SIZE\": 4,\n",
    "        \"ALGO\": \"MADDPG\",  # Algorithm\n",
    "        # Swap image channels dimension from last to first [H, W, C] -> [C, H, W]\n",
    "        \"CHANNELS_LAST\": False,\n",
    "        \"BATCH_SIZE\": 32,  # Batch size\n",
    "        \"LR_ACTOR\": 0.001,  # Learning rate\n",
    "        \"LR_CRITIC\": 0.001,  # Learning rate\n",
    "        \"GAMMA\": 0.97,  # Discount factor\n",
    "        \"MEMORY_SIZE\": 100000,  # Max memory buffer size\n",
    "        \"LEARN_STEP\": 5,  # Learning frequency\n",
    "        \"TAU\": 0.01,  # For soft update of target parameters\n",
    "        \"POLICY_FREQ\": 2,  # Policy frequnecy\n",
    "    }\n",
    "\n",
    "    # Define the simple speaker listener environment as a parallel environment\n",
    "    env = simple_world_comm_v3.parallel_env(continuous_actions=True)\n",
    "    env.reset()\n",
    "\n",
    "    # Configure the multi-agent algo input arguments\n",
    "    try:\n",
    "        state_dim = [env.observation_space(agent).n for agent in env.agents]\n",
    "        one_hot = True\n",
    "    except Exception:\n",
    "        state_dim = [env.observation_space(agent).shape for agent in env.agents]\n",
    "        one_hot = False\n",
    "    try:\n",
    "        action_dim = [env.action_space(agent).n for agent in env.agents]\n",
    "        INIT_HP[\"DISCRETE_ACTIONS\"] = True\n",
    "        INIT_HP[\"MAX_ACTION\"] = None\n",
    "        INIT_HP[\"MIN_ACTION\"] = None\n",
    "    except Exception:\n",
    "        action_dim = [env.action_space(agent).shape[0] for agent in env.agents]\n",
    "        INIT_HP[\"DISCRETE_ACTIONS\"] = False\n",
    "        INIT_HP[\"MAX_ACTION\"] = [env.action_space(agent).high for agent in env.agents]\n",
    "        INIT_HP[\"MIN_ACTION\"] = [env.action_space(agent).low for agent in env.agents]\n",
    "\n",
    "    # Not applicable to MPE environments, used when images are used for observations (Atari environments)\n",
    "    if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "        state_dim = [\n",
    "            (state_dim[2], state_dim[0], state_dim[1]) for state_dim in state_dim\n",
    "        ]\n",
    "\n",
    "    # Append number of agents and agent IDs to the initial hyperparameter dictionary\n",
    "    INIT_HP[\"N_AGENTS\"] = env.num_agents\n",
    "    INIT_HP[\"AGENT_IDS\"] = env.agents\n",
    "\n",
    "    # Create a population ready for evolutionary hyper-parameter optimisation\n",
    "    pop = initialPopulation(\n",
    "        INIT_HP[\"ALGO\"],\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        one_hot,\n",
    "        NET_CONFIG,\n",
    "        INIT_HP,\n",
    "        population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Configure the multi-agent replay buffer\n",
    "    field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "    memory = MultiAgentReplayBuffer(\n",
    "        INIT_HP[\"MEMORY_SIZE\"],\n",
    "        field_names=field_names,\n",
    "        agent_ids=INIT_HP[\"AGENT_IDS\"],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Instantiate a tournament selection object (used for HPO)\n",
    "    tournament = TournamentSelection(\n",
    "        tournament_size=2,  # Tournament selection size\n",
    "        elitism=True,  # Elitism in tournament selection\n",
    "        population_size=INIT_HP[\"POPULATION_SIZE\"],  # Population size\n",
    "        evo_step=1,\n",
    "    )  # Evaluate using last N fitness scores\n",
    "\n",
    "    # Instantiate a mutations object (used for HPO)\n",
    "    mutations = Mutations(\n",
    "        algo=INIT_HP[\"ALGO\"],\n",
    "        no_mutation=0.2,  # Probability of no mutation\n",
    "        architecture=0.2,  # Probability of architecture mutation\n",
    "        new_layer_prob=0.2,  # Probability of new layer mutation\n",
    "        parameters=0.2,  # Probability of parameter mutation\n",
    "        activation=0,  # Probability of activation function mutation\n",
    "        rl_hp=0.2,  # Probability of RL hyperparameter mutation\n",
    "        rl_hp_selection=[\n",
    "            \"lr\",\n",
    "            \"learn_step\",\n",
    "            \"batch_size\",\n",
    "        ],  # RL hyperparams selected for mutation\n",
    "        mutation_sd=0.1,  # Mutation strength\n",
    "        agent_ids=INIT_HP[\"AGENT_IDS\"],\n",
    "        arch=NET_CONFIG[\"arch\"],\n",
    "        rand_seed=1,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Define training loop parameters\n",
    "    max_episodes = 1000  # Total episodes (default: 6000)\n",
    "    max_steps = 25  # Maximum steps to take in each episode\n",
    "    epsilon = 1.0  # Starting epsilon value\n",
    "    eps_end = 0.1  # Final epsilon value\n",
    "    eps_decay = 0.995  # Epsilon decay\n",
    "    evo_epochs = 20  # Evolution frequency\n",
    "    evo_loop = 1  # Number of evaluation episodes\n",
    "    test_epochs = 10  # Test frequnecy\n",
    "    elite = pop[0]  # Assign a placeholder \"elite\" agent\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Training loop\n",
    "    for idx_epi in trange(max_episodes):\n",
    "        for agent in pop:  # Loop through population\n",
    "            state, info = env.reset()  # Reset environment at start of episode\n",
    "            agent_reward = {agent_id: 0 for agent_id in env.agents}\n",
    "            if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "                state = {\n",
    "                    agent_id: np.moveaxis(np.expand_dims(s, 0), [-1], [-3])\n",
    "                    for agent_id, s in state.items()\n",
    "                }\n",
    "\n",
    "            for _ in range(max_steps):\n",
    "                agent_mask = info[\"agent_mask\"] if \"agent_mask\" in info.keys() else None\n",
    "                env_defined_actions = (\n",
    "                    info[\"env_defined_actions\"]\n",
    "                    if \"env_defined_actions\" in info.keys()\n",
    "                    else None\n",
    "                )\n",
    "\n",
    "                # Get next action from agent\n",
    "                cont_actions, discrete_action = agent.getAction(\n",
    "                    state, epsilon, agent_mask, env_defined_actions\n",
    "                )\n",
    "                if agent.discrete_actions:\n",
    "                    action = discrete_action\n",
    "                else:\n",
    "                    action = cont_actions\n",
    "\n",
    "                next_state, reward, termination, truncation, info = env.step(\n",
    "                    action\n",
    "                )  # Act in environment\n",
    "\n",
    "                # Image processing if necessary for the environment\n",
    "                if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "                    state = {agent_id: np.squeeze(s) for agent_id, s in state.items()}\n",
    "                    next_state = {\n",
    "                        agent_id: np.moveaxis(ns, [-1], [-3])\n",
    "                        for agent_id, ns in next_state.items()\n",
    "                    }\n",
    "\n",
    "                # Save experiences to replay buffer\n",
    "                memory.save2memory(state, cont_actions, reward, next_state, termination)\n",
    "\n",
    "                # Collect the reward\n",
    "                for agent_id, r in reward.items():\n",
    "                    agent_reward[agent_id] += r\n",
    "\n",
    "                # Learn according to learning frequency\n",
    "                if (memory.counter % agent.learn_step == 0) and (\n",
    "                        len(memory) >= agent.batch_size\n",
    "                ):\n",
    "                    experiences = memory.sample(\n",
    "                        agent.batch_size\n",
    "                    )  # Sample replay buffer\n",
    "                    agent.learn(experiences)  # Learn according to agent's RL algorithm\n",
    "\n",
    "                # Update the state\n",
    "                if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "                    next_state = {\n",
    "                        agent_id: np.expand_dims(ns, 0)\n",
    "                        for agent_id, ns in next_state.items()\n",
    "                    }\n",
    "                state = next_state\n",
    "\n",
    "                # Stop episode if any agents have terminated\n",
    "                if any(truncation.values()) or any(termination.values()):\n",
    "                    break\n",
    "\n",
    "            # Save the total episode reward\n",
    "            score = sum(agent_reward.values())\n",
    "            agent.scores.append(score)\n",
    "\n",
    "        # Update epsilon for exploration\n",
    "        epsilon = max(eps_end, epsilon * eps_decay)\n",
    "\n",
    "        # Now evolve population if necessary\n",
    "        if (idx_epi + 1) % evo_epochs == 0:\n",
    "            elite, pop = tournament.select(pop)\n",
    "            pop = mutations.mutation(pop)\n",
    "\n",
    "        if idx_epi % test_epochs == 0:\n",
    "            evaluate(elite, env, idx_epi, results)\n",
    "    # save the training data        \n",
    "    with open(env.__str__() + \".json\", \"w\") as file:\n",
    "        json.dump(results, file, indent=4)\n",
    "        file.write(\"\\n\")\n",
    "        \n",
    "    path = \"./models/MADDPG/\" + env.__str__()\n",
    "    filename = \"MADDPG_trained_agent.pt\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    save_path = os.path.join(path, filename)\n",
    "    elite.saveCheckpoint(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b9b30-53d4-4a4d-8e3e-bbc0aa94703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the learning curve\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot(file_name,env = \"\"):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    with open(file_name, \"r\") as file:\n",
    "        data = json.load(file)[0:100]\n",
    "        \n",
    "    agent_rewards = {}\n",
    "    agent_rewards_dict = data[0][\"AgentReward\"]\n",
    "    for agent, rewards in agent_rewards_dict.items():\n",
    "        agent_rewards[agent] = []\n",
    "\n",
    "    episode_rewards = []\n",
    "    for epi_data in data:\n",
    "        episode_reward = epi_data[\"EpisodeReward\"]\n",
    "        episode_rewards.append(sum(episode_reward) / len(episode_reward))\n",
    "        \n",
    "        agent_rewards_dict = epi_data[\"AgentReward\"] # AdversaryRewards\n",
    "        \n",
    "        for agent, rewards in agent_rewards_dict.items():\n",
    "            average_reward = sum(rewards) / len(rewards)\n",
    "            agent_rewards[agent].append(average_reward)\n",
    "    x = range(0,len(episode_rewards)*10,10)\n",
    "    \n",
    "    # if you want to show the episode_rewards\n",
    "    # stds = [np.std(epi_data[\"EpisodeReward\"]) for epi_data in data]\n",
    "    # errors = [1.96 * std / np.sqrt(10) for std in stds]\n",
    "    # lower = [x - error for x,error in zip(episode_rewards,errors)]\n",
    "    # upper = [x + error for x,error in zip(episode_rewards,errors)]\n",
    "    # plt.fill_between(range(len(episode_rewards)), lower, upper, alpha=0.7)\n",
    "    # plt.plot(x, episode_rewards, label='Episode_rewards')\n",
    "    # print(\"episode_rewards\",episode_rewards)\n",
    "\n",
    "    \n",
    "    for agent in agent_rewards.keys():\n",
    "        plt.plot(x, agent_rewards[agent], label=agent)\n",
    "        stds = [np.std(epi_data[\"AgentReward\"][agent]) for epi_data in data]\n",
    "        errors = [1.96 * std / np.sqrt(10) for std in stds]\n",
    "        lower = [x - error for x,error in zip(agent_rewards[agent],errors)]\n",
    "        upper = [x + error for x,error in zip(agent_rewards[agent],errors)]\n",
    "        plt.fill_between(x, lower, upper, alpha=0.3)\n",
    "        # print(agent,agent_rewards[agent])\n",
    "        \n",
    "    # plt.axhline(y=-5, color='gray', linestyle='--')\n",
    "    # plt.axhline(y=5, color='gray', linestyle='--')\n",
    "    plt.title(env)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.grid()\n",
    "    plt.savefig(env+'.png')\n",
    "    plt.show()\n",
    "\n",
    "filename = \"simple_world_comm_v3.json\"\n",
    "plot(filename,filename.replace(\".json\",''))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
